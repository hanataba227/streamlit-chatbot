# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ì ¸ì˜¤ê¸°
import os
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="Chat", page_icon="ğŸ’¬")

# í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# API í‚¤ê°€ ì—†ìœ¼ë©´ ì—ëŸ¬ í‘œì‹œí•˜ê³  ì¤‘ë‹¨
if not api_key:
    st.error("OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=api_key)

def get_llm_response(messages, placeholder):
    """AIë¡œë¶€í„° ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ë°›ì•„ì˜¤ëŠ” í•¨ìˆ˜"""
    response = ""  # ì „ì²´ ì‘ë‹µì„ ì €ì¥í•  ë³€ìˆ˜
    
    # OpenAI API í˜¸ì¶œ (ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ)
    stream = client.chat.completions.create(
        model="gpt-4o-mini", 
        messages=messages, 
        stream=True
    )
    
    # ìŠ¤íŠ¸ë¦¼ì—ì„œ ê° ì²­í¬ë¥¼ ë°›ì•„ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ í™”ë©´ì— í‘œì‹œ
    for chunk in stream:
        content = chunk.choices[0].delta.content
        if content:  # ë‚´ìš©ì´ ìˆìœ¼ë©´
            response += content
            placeholder.markdown(response)  # í™”ë©´ì— ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
    
    return response

# ì„¸ì…˜ ìƒíƒœì— ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "system", "content": "ë‹¹ì‹ ì€ ê°„ê²°í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."}
    ]

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")

if user_input:  # ì‚¬ìš©ìê°€ ë©”ì‹œì§€ë¥¼ ì…ë ¥í–ˆìœ¼ë©´
    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": user_input})
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ í™”ë©´ì— í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(user_input)

    # AI ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
    with st.chat_message("assistant"):
        response_placeholder = st.empty()  # ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ë¹ˆ ê³µê°„
        ai_response = get_llm_response(st.session_state.messages, response_placeholder)
